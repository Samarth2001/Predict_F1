# config/default_config.yaml

# ============================================================================
# GENERAL CONFIGURATION
# ============================================================================
general:
  model_version: "2.0.0"
  random_state: 42
  enable_feature_selection: true
  max_features_selection: 100
  # Set to your desired logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_level: "INFO"

# ============================================================================
# PATH CONFIGURATION
# All paths are relative to the project's base directory.
# ============================================================================
paths:
  data_dir: "f1_data_real"
  cache_dir: "f1_data_real/ff1_cache"
  models_dir: "models"
  logs_dir: "logs"
  predictions_dir: "predictions"
  evaluation_dir: "evaluation"
  visualizations_dir: "visualizations"
  reports_dir: "reports"
  races_csv: "f1_data_real/all_races_data.csv"
  quali_csv: "f1_data_real/all_quali_data.csv"
  weather_csv: "f1_data_real/weather_data.csv"
  log_file_prefix: "f1_predictor"

# ============================================================================
# DATA COLLECTION CONFIGURATION
# ============================================================================
data_collection:
  start_year: 2018
  # 'auto' will use the current year
  current_season: 'auto'
  end_year: 'auto'
  # Settings for FastF1 API interaction
  fastf1:
    fetch_delay: 1.5
    api_retry_delay: 60
    api_max_retries: 5
    timeout_seconds: 45
    cache_expiry_hours: 24
    parallel_sessions: 4
  # Configuration for external APIs
  external_apis:
    # Sign up for a free key at https://openweathermap.org/
    openweathermap_api_key: "YOUR_API_KEY_HERE"
    ergast_api_base_url: "http://ergast.com/api/f1"
    rate_limit_per_minute: 60

# ============================================================================
# FEATURE ENGINEERING CONFIGURATION
# ============================================================================
feature_engineering:
  rolling_windows:
    short: 3
    medium: 5
    long: 10
    season: 23
  min_races_for_driver_features: 5
  min_races_for_team_features: 3
  finished_statuses: ['Finished', '+1 Lap', '+2 Laps', '+3 Laps', '+4 Laps', '+5 Laps']
  dnf_statuses: ['Accident', 'Collision', 'Engine', 'Gearbox', 'Hydraulics', 'Transmission', 'Electrical', 'Brakes', 'Suspension', 'Power Unit', 'Clutch', 'Wheel', 'Fuel System']
  circuit_categories:
    street: ['Monaco Grand Prix', 'Singapore Grand Prix', 'Azerbaijan Grand Prix', 'Las Vegas Grand Prix', 'Miami Grand Prix', 'Saudi Arabian Grand Prix']
    high_speed: ['Italian Grand Prix', 'Belgian Grand Prix', 'British Grand Prix', 'Japanese Grand Prix', 'Bahrain Grand Prix']
    technical: ['Hungarian Grand Prix', 'Spanish Grand Prix', 'Abu Dhabi Grand Prix']
  weather_impact:
    rain_probability_threshold: 30
    optimal_temp_range: [20, 30]
    wind_speed_threshold: 25

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
models:
  # 'ensemble' is recommended for best performance
  default_model_type: "ensemble"
  available: ['lightgbm', 'xgboost', 'random_forest', 'ensemble']
  
  # Parameters for individual models
  lightgbm_params:
    objective: 'regression_l1'
    metric: 'mae'
    n_estimators: 2000
    learning_rate: 0.02
    feature_fraction: 0.8
    bagging_fraction: 0.8
    bagging_freq: 1
    lambda_l1: 0.1
    lambda_l2: 0.1
    num_leaves: 31
    verbose: -1
    n_jobs: -1
    seed: 42
    boosting_type: 'gbdt'

  xgboost_params:
    objective: 'reg:squarederror'
    eval_metric: 'mae'
    n_estimators: 2000
    learning_rate: 0.02
    max_depth: 7
    subsample: 0.8
    colsample_bytree: 0.8
    gamma: 0.1
    random_state: 42
    n_jobs: -1

  random_forest_params:
    n_estimators: 500
    max_depth: 15
    min_samples_split: 5
    min_samples_leaf: 2
    random_state: 42
    n_jobs: -1

  # Configuration for the ensemble model
  ensemble_config:
    base_models: ['lightgbm', 'xgboost']
    # Weights will be optimized if optimize_weights is true
    weights: [0.5, 0.5]
    optimize_weights: true

# ============================================================================
# TRAINING AND VALIDATION CONFIGURATION
# ============================================================================
training:
  # Configuration for train-validation-test split
  split_config:
    test_size: 0.15
    validation_size: 0.15
    time_aware_split: true
    stratify_by: ['Year']
  
  # Configuration for cross-validation
  cv_config:
    cv_folds: 5
    cv_strategy: 'time_series' # 'time_series' or 'stratified'
    cv_group_by: 'Year'
  
  # Configuration for model training process
  training_config:
    early_stopping_rounds: 100
    early_stopping_metric: 'mae'

# ============================================================================
# HYPERPARAMETER OPTIMIZATION (OPTUNA)
# ============================================================================
hyperparameter_optimization:
  enabled: true
  n_trials: 100
  timeout_hours: 2
  optimization_metric: 'mae'
  optimization_direction: 'minimize'
  
  # Search spaces for hyperparameters
  param_search_spaces:
    lightgbm:
      n_estimators: [1000, 5000]
      learning_rate: [0.01, 0.05]
      max_depth: [7, 15]
      num_leaves: [31, 127]
    xgboost:
      n_estimators: [1000, 5000]
      learning_rate: [0.01, 0.05]
      max_depth: [7, 15]

# ============================================================================
# PREDICTION WORKFLOW CONFIGURATION
# ============================================================================
prediction:
  # Confidence thresholds for categorizing predictions
  confidence_thresholds:
    high: 0.80
    medium: 0.60
    low: 0.40
  
  # Default values for features if they are missing during prediction
  imputation_defaults:
    Grid_Pos: 15.0
    Quali_Pos: 15.0
    Weather_Temp: 25.0

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  # Metrics to calculate for evaluating model performance
  metrics: ['mae', 'rmse', 'r2', 'spearman_correlation', 'top3_accuracy', 'top5_accuracy']
  # Hold out one or more recent years for final testing
  holdout_years: [2024]
  # Number of iterations for bootstrap confidence intervals
  bootstrap_iterations: 1000 